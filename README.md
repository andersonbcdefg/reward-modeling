# reward-modeling
Train reward models for reinforcement learning from human feedback (RLHF). I use a long-context DeBERTa-v3 model, which has already been pretrained with a masked language modeling objective and finetuned on the [tasksource](https://github.com/sileod/tasksource) dataset, a massive multitask classification dataset. I fine-tune it further on over 500,000 pairs samples of human preference data.
