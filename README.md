# reward-modeling
Train reward models designed for human preference learning (e.g. RLHF) for language modeling. To start with, I use a long-context DeBERTa-v3 model, which has already been pretrained with a masked language modeling objective and finetuned on the [tasksource](https://github.com/sileod/tasksource) dataset, a massive multitask classification dataset. My goal with these experiments (which are not yet completed / written up) is to do ablations to understand how different preference datasets contribute to the performance of a reward model.

# Existing Preference Learning Datasets
These datasets are designed out-of-the-box for reward modeling. 
* [Anthropic hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)
* [Stanford SHP](https://huggingface.co/datasets/stanfordnlp/SHP)
* [WebGPT comparisons](https://huggingface.co/datasets/openai/webgpt_comparisons)
* [Dahoas/synthetic-instruct-gptj-pairwise](https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise)
* [Alpaca Farm](https://huggingface.co/datasets/tatsu-lab/alpaca_farm) (Human-ranked and GPT-4-ranked subsets)

At the time of most of my experiments, these are basically the bulk of publicly-available data for pairwise preference modeling. I personally believe that the quality is not great, and this underlies my motivation for experimenting with synthetic data for reward modeling. I don't think any current open research efforts towards RLHF / alignment have any chance of competing with OpenAI, Anthropic, or now even Meta's Llama-2-Chat models, simply by virtue of the poor quality of publicly available data, and the investment these companies have put into collecting high-quality human feedback in private.

Anthropic's HH-RLHF is the "gold standard" of publicly-available human preference data. Stanford SHP data is scraped from Reddit, and therefore doesn't align well with the kind of conversations people have with language models (it's people responding to each others' comments on Reddit), and I have anecdotally observed weird results when testing out the models Stanford trained on this data. WebGPT data is specific to a LLM that can browse the web and cite its sources, so using it for general human preference modeling for LLMs is a distribution shift. The GPTJ-Instruct synthetic dataset has major data quality issues that make it basically worthless for reward modeling (see OpenAssistant's note in [this repo](https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large)). Preference data from the Alpaca Farm project is newer so I do not yet have a strong judgment on its quality, but human-labeled data is pretty small relative to their amount of GPT-4-labeled data.


# Synthetic Data Generation
To see if it's possible to improve on existing data without hiring tons of labelers, I experiment with creating new reward modeling data using LLMs. I gather completions for prompts by prompting language models, then using language models to rank the responses on a set of basic principles for what makes a good completion. The result is a dataset of partially or fully model-generated responses to prompts, where each prompt has a "chosen" response and a "rejected" response. Many of these datasets also include a column for the "justification" of why one response was preferred to another. Scripts for doing this are all in the `synthetic_data_scripts` directory. Sorry in advance that they're a mess, I never really got a solid, repeatable interface for generating data and was doing it messily in notebooks.
* __Dolly 15k:__ Dolly is a dataset of instructions and completions from Databricks. I only include instructions that don't have an accompanying 'input'. I collect responses from a couple of open-source models (RedPajama-INCITE 3B and Pythia-12B), and also keep the original human-written responses. I use GPT-4 to rank two AI-generated responses first, then to rank the better AI-generated response vs. the human-written response. Many of the AI-generated responses are of dubious quality, e.g. cutting off in the middle of a sentence. However, this dataset could still be useful, since (a) it's important for preference models to understand what makes a response bad/useless; (b) all completions in this dataset are generated by permissive models; and (c) the preference data is based on realistic instructions that a user might actually prompt a language model with. [Access the dataset here.](https://huggingface.co/datasets/andersonbcdefg/dolly_reward_modeling_pairwise) 
* __GPTeacher:__ GPTeacher is a dataset of (AI-generated) instructions and completions from GPT-4, describing typical tasks/questions one might ask a language model. As with Dolly, I only use instructions that are self-contained (don't rely on an 'input'), and keep the GPT-4 generated responses from the original GPTeacher dataset. I generate new completions for comparison using the OpenAI API (`text-davinci-003`). Since this is a much larger dataset, I use GPT-3.5 to rank the responses, so it is possible that the rankings for this dataset are less reliable. Here, we are generally ranking higher-quality completions, since GPT-4 and `text-davinci-003` are both quite capable. This dataset could be helpful for providing comparisons between higher-quality completions, but be careful about licensing & commercial use, since the OpenAI ToS prohibits using outputs from the API for developing AI models that compete with OpenAI. (Whether just training a preference model counts is unclear; I am not a lawyer.) [Access the dataset here.](https://huggingface.co/datasets/andersonbcdefg/gpteacher_reward_modeling_pairwise)
* __Anthropic Red-Teaming Data:__ When it comes to preference learning, it's important to not only consider "good" uses of language models, but also how users may try to misuse and abuse them. This dataset from Anthropic contains transcripts of conversations where users try to get language models to generate harmful, discriminatory, or otherwise problematic content. I take the first user message from each of these conversations, which often (though not always) is where the problematic request occurs. I attempt to generate problematic completions for these prompts using `text-curie-001` and the base `davinci` model, since these have not been trained as extensively to avoid outputting harmful content. I collect more completions using GPT-3.5, since it is likely to avoid problematic content, and thus create good comparison data. Finally, I use GPT-3.5 to rank the completions, as this is a large dataset and GPT-4 is slow and expensive. This dataset is useful to train a preference model that understands to prefer harmless completions to attempts to misuse models. The same disclaimers about using OpenAI-generated data apply here. I release two versions: one which excludes "as an AI language model..." responses (this may or may not be behavior you want to train into a model), which you can access [here](https://huggingface.co/datasets/andersonbcdefg/red_teaming_reward_modeling_pairwise_no_as_an_ai), and another which does not filter those out (access [here](https://huggingface.co/datasets/andersonbcdefg/redteaming_eval_pairwise).
* __ShareGPT:__ [ShareGPT](theblackcat102/sharegpt-english) is a dataset of real conversations between users and ChatGPT. It's a good candidate for reward modeling because it captures realistic uses of LLM assistants "in the wild". I use the prompts from this dataset to generate completions and rank them with the OpenAI API, similar to the above datasets. I also release two versions of this, an [unfiltered one here](https://huggingface.co/datasets/andersonbcdefg/sharegpt_reward_modeling_pairwise), and a [filtered one](https://huggingface.co/datasets/andersonbcdefg/sharegpt_reward_modeling_pairwise_no_as_an_ai) that excludes "as an AI language model" responses.
* __synthetic-instruct-gptj-pairwise:__ This is a dataset of preference data from Alex Havrilla. It's designed for reward modeling already, and I experimented with using the data as-is, but I found (as did some folks working on Open Assistant) that this data has some weird artefacts that make it really easy for LLMs to tell the chosen completion from the rejected one. So, I built another version of this dataset where GPT-3.5-turbo was prompted to just paraphrase the completions from this dataset, but removing any unusual whitespace or punctuation (which I believe are to blame for the poor quality of this dataset). Access the paraphrased dataset [here](https://huggingface.co/datasets/andersonbcdefg/synthetic_gptj_paraphrased).


The synthetic Dolly dataset is licensed under CC-by-SA 3.0 license, as any remix of the Dolly dataset must be licensed under identical terms. The other datasets are mostly MIT-licensed, and I don't care what you do with them. However, it is your responsibility to avoid violating the OpenAI ToS in your use of this data. (The ToS do not prohibit creating and publishing synthetic data; it is your responsibility not to misuse it!)

# Evaluating Preference Models
I use a few datasets for evaluation:
- alpaca gpt4 preference data from Alpaca Farm
- alpaca human preference data from Alpaca Farm
- test set of Anthropic hh-rlhf
- test set of Stanford SHP
- some synthetic data from red-teaming prompts

Evaluation metric is simple: Score each pair with the reward model, and consider the result correct if it prefers the "preferred" response over the "dispreferred" response.

# Experiments (work in progress)
Goal is to start from Anthropic hh-rlhf, which is a standard dataset for training preference models, and see how other datasets (especially synthetic datasets) can be used to improve performance. I expect some of these synthetic datasets to help quite a bit, as they capture a diversity of ways that people use (and misuse!) language models, which may not be captured in the relatively small Anthropic dataset. I'm interested in whether the synthetic data is more helpful than Stanford-SHP, which is really large, but seems low-quality to me for these purposes (Reddit karma is not a great proxy for the thing we actually want). You can see the experiments I've done so far here: https://wandb.ai/andersonbcdefg/reward_model_ablations

# Future Work
If I get around to training my own better BERT model, I may swap that in for the DeBERTA. DeBERTa is basically the best encoder model and has a good tokenizer, but it's harder to scale the sequence length without Flash Attention. On the other hand, the newer MosaicBERT may have Flash Attention, but it's trained with a bad tokenizer (uncased WordPiece), which might be fine for GLUE, but is limited when we're trying to score (cased, more general) text generated by LLMs (let alone code!). I'd much rather have a nice SentencePiece BERT trained on 600B-1T tokens, but that doesn't exist right now. :( Might also work to pop the encoder out of a T5 model and train it with MLM objective 'til it reaches BERT performance. We'll see.
