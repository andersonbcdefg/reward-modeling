# reward-modeling
Train reward models for reinforcement learning from human feedback (RLHF). To start with, I use a long-context DeBERTa-v3 model, which has already been pretrained with a masked language modeling objective and finetuned on the [tasksource](https://github.com/sileod/tasksource) dataset, a massive multitask classification dataset.

# Preference Learning Datasets
* Anthropic hh-rlhf
* Stanford SHP
* WebGPT comparisons
* Dahoas/synthetic-instruct-gptj-pairwise

# Synthetic Data Generation (In Progress)
I also experiment with generating synthetic data by prompting language models, gathering completions for prompts, and then using language models to rank the responses on a set of basic principles for what makes a good completion. This work is still in progress, but datasets used for generating completions are as follows:
* __Dolly 15k:__ A dataset of instructions and completions from Databricks. I only include instructions that don't have an accompanying 'input'. I collect responses from a couple of open-source models (RedPajama-INCITE 3B and Pythia-12B), and also keep the original human-written responses. I use GPT-4 to rank two AI-generated responses first, then to rank the better AI-generated response vs. the human-written response. Many of the AI-generated responses are of dubious quality, cutting off in the middle of a sentence. However, I believe this dataset is still useful, since (a) it's important for preference models to understand what makes a response bad/useless; (b) all completions in this dataset are generated by permissive models; and (c) the preference data is based on realistic instructions that a user might actually prompt a language model with.
* __GPTeacher:__ GPTeacher is a dataset of (AI-generated) instructions and completions from GPT-4, describing typical tasks/questions one might ask a language model. As with Dolly, I only use instructions that are self-contained (don't rely on an 'input'), and keep the GPT-4 generated responses. I generate new completions for comparison using the OpenAI API (`text-davinci-003`). Since this is a much larger dataset, I use GPT-3.5 to rank the responses, so it is possible that the rankings for this dataset are less reliable. Here, we are also generally ranking higher-quality completions, since GPT-4 and `text-davinci-003` are both quite capable. This dataset could be helpful for providing comparisons between higher-quality completions, but be careful about licensing & commercial use, since the OpenAI ToS prohibits using outputs from the API for developing AI models that compete with OpenAI. (Whether just training a preference model counts is unclear; I am not a lawyer.)
* __Anthropic Red-Teaming Data:__ When it comes to preference learning, it's important to not only consider "good" uses of language models, but also how users may try to misuse and abuse them. This dataset from Anthropic contains transcripts of conversations where users try to get language models to generate harmful, discriminatory, or otherwise problematic content. I take the first user message from each of these conversations, which often (though not always) is where the problematic request occurs. I attempt to generate problematic completions for these prompts using `text-curie-001` and the base `davinci` model, since these have not been trained as extensively to avoid outputting harmful content. I collect more completions using GPT-3.5, since it is likely to avoid problematic content, and thus create good comparison data. Finally, I use GPT-3.5 to rank the completions, as this is a large dataset and GPT-4 is slow and expensive. This dataset is useful to train a preference model that understands to prefer harmless completions to attempts to misuse models. The same disclaimers about using OpenAI-generated data apply here.

The synthetic Dolly dataset is licensed under CC-by-SA 3.0 license, as any remix of the Dolly dataset must be licensed under identical terms. The other datasets are MIT-licensed, since I own these outputs. However, it is your responsibility to avoid violating the OpenAI ToS in your use of this data. (The ToS do not prohibit creating and publishing synthetic data; it is your responsibility not to misuse it!)

# Evaluating Preference Models
I use a few datasets for evaluation:
- alpaca gpt4 preference data
- alpaca human preference data
- test set of Anthropic hh-rlhf
- test set of Stanford SHP

Evaluation metric is simple: Score each pair with the reward model, and consider the result correct if it prefers the "preferred" response over the "dispreferred" response.

# Experiments (work in progress)
Goal is to start from Anthropic hh-rlhf, which is a standard dataset for training preference models, and see how other datasets(especially synthetic datasets) can be used to improve performance. I expect some of these synthetic datasets to help quite a bit, as they capture a diversity of ways that people use (and misuse!) language models, which may not be captured in the relatively small Anthropic dataset. I'm interested in whether the synthetic data is more helpful than Stanford-SHP, which is really large, but seems low-quality to me for these purposes (Reddit karma is not a great proxy for the thing we actually want).