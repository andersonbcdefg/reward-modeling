{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:56:48.791315Z",
     "iopub.status.busy": "2023-05-20T15:56:48.790987Z",
     "iopub.status.idle": "2023-05-20T15:56:52.943898Z",
     "shell.execute_reply": "2023-05-20T15:56:52.942872Z",
     "shell.execute_reply.started": "2023-05-20T15:56:48.791287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade torch transformers datasets accelerate\n",
    "# !git clone https://github.com/andersonbcdefg/rewardmodeling.git\n",
    "!cd rewardmodeling && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-20T15:56:53.500316Z",
     "iopub.status.busy": "2023-05-20T15:56:53.499874Z",
     "iopub.status.idle": "2023-05-20T15:56:58.172653Z",
     "shell.execute_reply": "2023-05-20T15:56:58.171689Z",
     "shell.execute_reply.started": "2023-05-20T15:56:53.500274Z"
    },
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "2bab3caa-e156-4635-bc21-53031ebea60d",
     "kernelId": ""
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DefaultDataCollator\n",
    "from accelerate import Accelerator\n",
    "from rewardmodeling.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-05-20T15:56:58.174524Z",
     "iopub.status.busy": "2023-05-20T15:56:58.174080Z",
     "iopub.status.idle": "2023-05-20T15:57:00.827138Z",
     "shell.execute_reply": "2023-05-20T15:57:00.826528Z",
     "shell.execute_reply.started": "2023-05-20T15:56:58.174506Z"
    },
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "631deddf-30f0-45f1-84ab-e5f4c510c500",
     "kernelId": ""
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at sileod/deberta-v3-base-tasksource-nli and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# use a deberta-v3 that has already been finetuned on massively-multitask dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"sileod/deberta-v3-base-tasksource-nli\", num_labels=1, ignore_mismatched_sizes=True)\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:57:02.055346Z",
     "iopub.status.busy": "2023-05-20T15:57:02.054840Z",
     "iopub.status.idle": "2023-05-20T15:57:02.059188Z",
     "shell.execute_reply": "2023-05-20T15:57:02.058597Z",
     "shell.execute_reply.started": "2023-05-20T15:57:02.055318Z"
    }
   },
   "outputs": [],
   "source": [
    "# freeze embeddings and first 8 layers. save memory!\n",
    "def freeze(module):\n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "freeze(model.deberta.embeddings)\n",
    "for layer in model.deberta.encoder.layer[:8]:\n",
    "    freeze(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:57:06.788417Z",
     "iopub.status.busy": "2023-05-20T15:57:06.787990Z",
     "iopub.status.idle": "2023-05-20T15:57:07.300744Z",
     "shell.execute_reply": "2023-05-20T15:57:07.300146Z",
     "shell.execute_reply.started": "2023-05-20T15:57:06.788393Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/andersonbcdefg___parquet/andersonbcdefg--reward-modeling-long-tokenized-5d7ae91cc6f67e21/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/andersonbcdefg___parquet/andersonbcdefg--reward-modeling-short-tokenized-6ef67474794d1513/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "# Get datasets and tokenize! To save time, I'm using pre-tokenized versions I uploaded to HuggingFace\n",
    "# datasets = get_combined_datasets()\n",
    "# data_long, data_short = datasets['long'], datasets['short']\n",
    "# data_long_tokenized = data_long.map(partial(tokenize_function, tokenizer=tokenizer, max_len=2048), \n",
    "#                                     batched=True, remove_columns=data_long.column_names)\n",
    "# data_short_tokenized = data_short.map(partial(tokenize_function, tokenizer=tokenizer, max_len=1024),\n",
    "#                                       batched=True, remove_columns=data_short.column_names)\n",
    "data_long_tokenized = load_dataset(\"andersonbcdefg/reward-modeling-long-tokenized\", split=\"train\")\n",
    "data_short_tokenized = load_dataset(\"andersonbcdefg/reward-modeling-short-tokenized\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:57:08.525544Z",
     "iopub.status.busy": "2023-05-20T15:57:08.525090Z",
     "iopub.status.idle": "2023-05-20T15:57:08.529391Z",
     "shell.execute_reply": "2023-05-20T15:57:08.528813Z",
     "shell.execute_reply.started": "2023-05-20T15:57:08.525518Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_dataloader = torch.utils.data.DataLoader(data_short_tokenized, batch_size=64, pin_memory=True, collate_fn=DefaultDataCollator(), num_workers=1)\n",
    "long_dataloader = torch.utils.data.DataLoader(data_long_tokenized, batch_size=16, pin_memory=True, collate_fn=DefaultDataCollator(), num_workers=1)\n",
    "# 2 epochs on shorter data, then finish with longer data so that RM can handle longer sequences!\n",
    "train_dataloader = chain(short_dataloader, long_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:57:14.157641Z",
     "iopub.status.busy": "2023-05-20T15:57:14.157062Z",
     "iopub.status.idle": "2023-05-20T15:57:16.106009Z",
     "shell.execute_reply": "2023-05-20T15:57:16.105451Z",
     "shell.execute_reply.started": "2023-05-20T15:57:14.157605Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model, optimizer, scheduler, accelerator\n",
    "model.to(torch.device(\"cuda\"))\n",
    "max_lr = 3.0e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(0.9, 0.95), fused=True)\n",
    "# constant learning rate with warmup\n",
    "scheduler_kwargs = {\n",
    "    \"max_lr\": max_lr,\n",
    "    \"total_steps\": len(short_dataloader) + len(long_dataloader) + 10,\n",
    "    \"pct_start\": 0.005,\n",
    "    \"div_factor\": 10,\n",
    "    \"final_div_factor\": 1,\n",
    "    \"anneal_strategy\": \"linear\",\n",
    "    \"three_phase\": False\n",
    "}\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, **scheduler_kwargs)\n",
    "accelerator = Accelerator(mixed_precision=\"bf16\")\n",
    "model, optimizer, train_dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:57:17.580147Z",
     "iopub.status.busy": "2023-05-20T15:57:17.579683Z",
     "iopub.status.idle": "2023-05-20T15:57:19.942913Z",
     "shell.execute_reply": "2023-05-20T15:57:19.942344Z",
     "shell.execute_reply.started": "2023-05-20T15:57:17.580122Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandersonbcdefg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20230520_155719-26fa4x64</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/andersonbcdefg/train_reward_model/runs/26fa4x64\" target=\"_blank\">solar-totem-11</a></strong> to <a href=\"https://wandb.ai/andersonbcdefg/train_reward_model\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/andersonbcdefg/train_reward_model/runs/26fa4x64?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5a9e1b3e20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"23aafc180deb281d60c00aaa76932952ce9fdf38\")\n",
    "wandb.init(\n",
    "    project=\"train_reward_model\",\n",
    "    config={\n",
    "        \"max_lr\": max_lr,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T15:57:25.640275Z",
     "iopub.status.busy": "2023-05-20T15:57:25.639564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint...\n"
     ]
    }
   ],
   "source": [
    "effective_batch_size = 64\n",
    "sample_count = 0\n",
    "optimizer_steps = 0\n",
    "save_every = 500\n",
    "\n",
    "model.train()\n",
    "for index, batch in enumerate(train_dataloader):\n",
    "    pref_ids, pref_mask, dispref_ids, dispref_mask = (\n",
    "        batch['preferred_input_ids'].to(accelerator.device), # bsz, seq_len\n",
    "        batch['preferred_attention_masks'].to(accelerator.device),\n",
    "        batch['dispreferred_input_ids'].to(accelerator.device),\n",
    "        batch['dispreferred_attention_masks'].to(accelerator.device)\n",
    "    )\n",
    "    bsz = pref_ids.shape[0]\n",
    "    sample_count += bsz\n",
    "    if sample_count < effective_batch_size:\n",
    "        with accelerator.no_sync(model):\n",
    "            pref_rewards, dispref_rewards = model(\n",
    "                torch.cat([pref_ids, dispref_ids], dim=0), \n",
    "                attention_mask=torch.cat([pref_mask, dispref_mask], dim=0)\n",
    "            ).logits.chunk(2, dim=0)\n",
    "            micro_batch_loss = -torch.log(torch.sigmoid(pref_rewards.view(-1) - dispref_rewards.view(-1))).sum() / bsz\n",
    "            accelerator.backward(micro_batch_loss)\n",
    "            # accelerator.clip_grad_norm_(model.parameters(), 1.0) # -- next time try 1.0\n",
    "        wandb.log({\"micro_batch_loss\": micro_batch_loss.item()})\n",
    "    else:\n",
    "        pref_rewards, dispref_rewards = model(\n",
    "            torch.cat([pref_ids, dispref_ids], dim=0), \n",
    "            attention_mask=torch.cat([pref_mask, dispref_mask], dim=0)\n",
    "        ).logits.chunk(2, dim=0)\n",
    "        micro_batch_loss = -torch.log(torch.sigmoid(pref_rewards.view(-1) - dispref_rewards.view(-1))).sum() / bsz\n",
    "        accelerator.backward(micro_batch_loss)\n",
    "        # accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        sample_count = 0\n",
    "        optimizer_steps += 1\n",
    "        wandb.log({\"micro_batch_loss\": micro_batch_loss.item()})\n",
    "    \n",
    "    if (index + 1) % save_every == 0:\n",
    "        print(\"Saving checkpoint...\")\n",
    "        accelerator.save_state(\"/storage\")\n",
    "    \n",
    "    scheduler.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T06:35:52.967943Z",
     "iopub.status.busy": "2023-05-20T06:35:52.967274Z",
     "iopub.status.idle": "2023-05-20T06:35:53.123505Z",
     "shell.execute_reply": "2023-05-20T06:35:53.122412Z",
     "shell.execute_reply.started": "2023-05-20T06:35:52.967916Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accelerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unwrapped_model \u001b[38;5;241m=\u001b[39m \u001b[43maccelerator\u001b[49m\u001b[38;5;241m.\u001b[39munwrap_model(model)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# state_dict = unwrapped_model.state_dict()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# torch.save(state_dict, \"/storage/rm_checkpoint_2.pt\")\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accelerator' is not defined"
     ]
    }
   ],
   "source": [
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "# state_dict = unwrapped_model.state_dict()\n",
    "# torch.save(state_dict, \"/storage/rm_checkpoint_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-c2868409ed9b2fce.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-672fbf2aaea8f228.arrow\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"test\").select(range(1000))\n",
    "eval_dataset = eval_dataset.map(\n",
    "    process_anthropic, remove_columns=eval_dataset.column_names\n",
    ")\n",
    "eval_dataset_tokenized = eval_dataset.map(partial(tokenize_function, tokenizer=tokenizer, max_len=1024), \n",
    "                                     batched=True, remove_columns=eval_dataset.column_names)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset_tokenized, batch_size=8, pin_memory=True, collate_fn=DefaultDataCollator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "unwrapped_model.load_state_dict(torch.load(\"/storage/rm_checkpoint.pt\"))\n",
    "unwrapped_model.eval()\n",
    "results = []\n",
    "for index, batch in enumerate(eval_dataloader):\n",
    "    pref_ids, pref_mask, dispref_ids, dispref_mask = (\n",
    "        batch['preferred_input_ids'].to(device), \n",
    "        batch['preferred_attention_masks'].to(device),\n",
    "        batch['dispreferred_input_ids'].to(device),\n",
    "        batch['dispreferred_attention_masks'].to(device)\n",
    "    )\n",
    "    pref_rewards = model(pref_ids, attention_mask=pref_mask).logits.view(-1)\n",
    "    dispref_rewards = model(dispref_ids, attention_mask=dispref_mask).logits.view(-1)\n",
    "    correct = (pref_rewards > dispref_rewards).long()\n",
    "    results.extend(list(correct.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best open-soure models tend to get around 0.75 accuracy if i remember correctly\n",
    "import numpy as np\n",
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
